{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad30508a-037d-4fe0-b39d-b074f574e44f",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "We tested dozens, maybe hundreds of ideas. Since training data was small, for each idea, we ran the local CV 10 times with 10 different K Fold random seeds and averaged the scores (that's 5 folds times 10 equals 50). Each change below increased CV average by at least 0.001\n",
    "\n",
    "1. Do not remove extra white space.\n",
    "\n",
    "The extra white space contains signal. For example if text is \"that's awesome!\" then selected text is awesome. However if text is \" that's awesome!\" then selected text is s awesome. The second example has extra white space in the beginning of text. And resultantly the selected text has an extra proceeding letter.\n",
    "\n",
    "2. Break apart common single tokens\n",
    "\n",
    "RoBERTa makes a single token for \"...\", so your model cannot chose \"fun.\" if the text is \"This is fun...\". So during preprocess, convert all single [...] tokens into three [.][.][.] tokens. Similarily, split \"..\", \"!!\", \"!!!\".\n",
    "\n",
    "3. Underestimate train targets\n",
    "\n",
    "Jaccard score is higher is you underestimate versus overestimate. Therefore if text is \" Matt loves ice cream\" and the selected text is \"t love\". Then train your model with selected text \"love\" not selected text \"Matt love\". All public notebook do the later, we suggest the former.\n",
    "\n",
    "4. Modified Question Answer head\n",
    "\n",
    "First predict the end index. Then concatenate the end index logits with RoBERTa last hidden layer to predict the start index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6875ffd2-c59f-45f2-a29a-d240482416f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROBERTA\n",
    "bert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "x = bert_model(q_id,attention_mask=q_mask,token_type_ids=q_type)\n",
    "\n",
    "# END INDEX HEAD\n",
    "x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "x2b = tf.keras.layers.Dense(1)(x2)\n",
    "x2 = tf.keras.layers.Flatten()(x2b)\n",
    "x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "# START INDEX HEAD\n",
    "x1 = tf.keras.layers.Concatenate()([x2b,x[0]])\n",
    "x1 = tf.keras.layers.Dropout(0.1)(x1) \n",
    "x1 = tf.keras.layers.Dense(1)(x1)\n",
    "x1 = tf.keras.layers.Flatten()(x1)\n",
    "x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "\n",
    "# MODEL\n",
    "model = tf.keras.models.Model(inputs=[q_id, q_mask, q_type], outputs=[x1,x2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1401284f-dc32-4e9d-9700-463ca99b32ab",
   "metadata": {},
   "source": [
    "5. Use label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1c24b-d2fd-4118-b142-b36ff9a418f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e740d2-09de-4cb7-9b1f-4d1cd9aa2153",
   "metadata": {},
   "source": [
    "6. Mask words\n",
    "7. Use data loader to randomly replace 5% of words with [mask] token 50264. Within your dataloader use the following code. We also maintain where the special tokens are so that they don't get replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df26853-eaca-4c7f-98a0-a61e9f540d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.uniform(0,1,ids.shape)\n",
    "ids[r<0.05] = 50264 \n",
    "ids[tru] = self.ids[indexes][tru]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25236528-adc8-43a7-bdf1-e483981bbc00",
   "metadata": {},
   "source": [
    "7. Decay learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976275d3-42f8-4aec-b1ed-d2e24ce68ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrfn(epoch):\n",
    "    dd = {0:4e-5,1:2e-5,2:1e-5,3:5e-6,4:2.5e-6}\n",
    "    return dd[epoch]\n",
    "lr = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b26e9c-a9fa-4bfa-960b-6772efb25914",
   "metadata": {},
   "source": [
    "8. Train each fold 100% data for submit\n",
    "\n",
    "10. After using normal 5 fold and early stopping, note how many epochs are optimal. Then for your LB submission, run your 5 folds with the fixed epoch number you found using 100% data each fold.\n",
    "\n",
    "9. Sample weight positive and negative\n",
    "\n",
    "In TensorFlow Keras it is easy to make certain training samples more important. The normal output from class DataGenerator(tf.keras.utils.Sequence) is (X,y). Instead output (X,y,w) where weight is the same shape as y. Then make w=2 for all the positive and negative targets and w=1 for all the neutral targets. Then train with the usual TensorFlow Keras calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b71d41-69e7-4ceb-bc4f-b21795796560",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_gen = DataGenerator()\n",
    "model.fit(t_gen)\n",
    "And volia! CV and LB increase 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27598133-2965-4c88-98a6-f87008e4b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Post process\n",
    "\n",
    "The above 9 changes already predict much of the noise. For example the above has no problem with the following 2 examples. Text is \" that's awesome!!!\" with selected text \"s awesome!\". And \" I'm thinking... wonderful.\" with selected text \". wonderful\". In each case, the model sees the leading double white space and extracts the single proceeding character.\n",
    "\n",
    "However the model cannot break a single letter off a word like text \"went fishing and loved it\" with selected text \"d loved\". This would require breaking a \"d\" off of the word \"and\". For these difficult cases, we use post process which increase CV 0.0025 and LB 0.0025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55682a60-d4ae-4b3a-abc3-33de5fdabafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT s=predicted, t=text, ex=sentiment\n",
    "# OUTPUT predicted with PP\n",
    "\n",
    "def applyPP(s,t,ex):\n",
    "\n",
    "    t1 = t.lower()\n",
    "    t2 = s.lower()\n",
    "\n",
    "    # CLEAN PREDICTED\n",
    "    b = 0\n",
    "    if len(t2)>=1:\n",
    "        if t2[0]==' ': \n",
    "            b = 1\n",
    "            t2 = t2[1:]\n",
    "    x = t1.find(t2)\n",
    "\n",
    "    # PREDICTED MUST BE SUBSET OF TEXT\n",
    "    if x==-1:\n",
    "        print('CANT FIND',k,x)\n",
    "        print(t1)\n",
    "        print(t2)\n",
    "        return s\n",
    "\n",
    "    # ADJUST FOR EXTRA WHITE SPACE\n",
    "    p = np.sum( np.array(t1[:x].split(' '))=='' )\n",
    "    if (p>2): \n",
    "        d = 0; f = 0\n",
    "        if p>3: \n",
    "            d=p-3\n",
    "        return t1[x-1-b-d:x+len(t2)]\n",
    "\n",
    "    # CLEAN BAD PREDICTIONS\n",
    "    if (len(t2)<=2)|(ex=='neutral'):\n",
    "        return t1\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad6437-33ff-46a2-a93b-445c1c2a7ae1",
   "metadata": {},
   "source": [
    "### Other ideas\n",
    "\n",
    "Our team tried tons of more ideas which may have worked if we spent more time to refine them. Below are some interesting things we tried:\n",
    "\n",
    "replacing **** with the original curse word.\n",
    "using part of speech information as an additional feature\n",
    "using NER model predictions as additional features\n",
    "compare test text with train text using Jaccard and use train selected text when jac >= 0.85 and text length >= 4 . (This gained 0.001 on public LB but didn't change private LB).\n",
    "pretrain with Sentiment140 dataset as MLM (masked language model)\n",
    "pseudo label Sentiment140 dataset and pretrain as QA (question answer model)\n",
    "Train a BERT to choose the best prediction from multiple BERT predictions.\n",
    "Stack BERTs. Append output from one BERT to the QA training data of another BERT.\n",
    "Tons of ensembling ideas like Jaccard expectation, softmax manipulations, voting ensembles, etc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
