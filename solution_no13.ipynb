{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9295ed29-00d7-4b5f-9b33-12932cc3e221",
   "metadata": {},
   "source": [
    "#### This is mostly the copied notebook submitted to kaggle here: https://www.kaggle.com/competitions/tweet-sentiment-extraction/discussion/159505\n",
    "\n",
    "Since training data was small, for each idea, we ran the local CV 10 times with 10 different K Fold random seeds and averaged the scores (that's 5 folds times 10 equals 50). Each change below increased CV average by at least 0.001\n",
    "\n",
    "1. Do not remove extra white space.\n",
    "\n",
    "The extra white space contains signal. For example if text is \"that's awesome!\" then selected text is awesome. However if text is \" that's awesome!\" then selected text is s awesome. The second example has extra white space in the beginning of text. And resultantly the selected text has an extra proceeding letter.\n",
    "\n",
    "2. Break apart common single tokens\n",
    "\n",
    "(https://www.kaggle.com/code/cdeotte/tensorflow-roberta-0-705/notebook\n",
    "RoBERTa makes a single token for \"...\", so your model cannot chose \"fun.\" if the text is \"This is fun...\". So during preprocess, convert all single [...] tokens into three [.][.][.] tokens. Similarily, split \"..\", \"!!\", \"!!!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5fc1ad-4555-4cc6-8b7b-5f3883760646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.4 (v3.10.4:9d38120e33, Mar 23 2022, 17:29:05) [Clang 13.0.0 (clang-1300.0.29.30)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae6a62ad-6400-4424-95e3-ca02f3df4850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 14:07:00.501918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-23 14:07:09.115631: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "from tensorflow import keras\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f57e24-f75f-446d-b9b0-3c3687408bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 96\n",
    "PATH = 'roBERTaFiles'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab=PATH+'/vocab-roberta-base.json', \n",
    "    merges=PATH+'/merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792eef61-fc8a-4ba7-acc1-cd4e6b8c61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd324558-2ed8-4d0d-b15d-c13c0fbf7572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We must tokenize the test data exactly the same as we tokenize the training data\n",
    "\n",
    "test = pd.read_csv('tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0e2bc-78b0-4c97-8c83-09e1b5f05481",
   "metadata": {},
   "source": [
    "2.1. Build roBERTa Model\n",
    "\n",
    "We use a pretrained roBERTa base model and add a custom question answer head. First tokens are input into bert_model and we use BERT's first output, i.e. x[0] below. These are embeddings of all input tokens and have shape (batch_size, MAX_LEN, 768). Next we apply tf.keras.layers.Conv1D(filters=1, kernel_size=1) and transform the embeddings into shape (batch_size, MAX_LEN, 1). We then flatten this and apply softmax, so our final output from x1 has shape (batch_size, MAX_LEN). These are one hot encodings of the start tokens indicies (for selected_text). And x2 are the end tokens indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82eb11cd-d3ac-4a0c-ba13-24c21a7556ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'/input/config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+\"/input/pretrained-roberta-base.h5\",config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss=keras.losses.CategoricalCrossentropy(), optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a55f4f-63af-4d05-817d-d0d61c203802",
   "metadata": {},
   "source": [
    "3. Underestimate train targets\n",
    "\n",
    "Jaccard score is higher is you underestimate versus overestimate. Therefore if text is \" Matt loves ice cream\" and the selected text is \"t love\". Then train your model with selected text \"love\" not selected text \"Matt love\". All public notebook do the later, we suggest the former.\n",
    "\n",
    "4. Modified Question Answer head\n",
    "\n",
    "First predict the end index. Then concatenate the end index logits with RoBERTa last hidden layer to predict the start index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f69b422b-03d4-4415-9103-3b676c593780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004fd4f-d7fe-472b-ac6a-96accfd31322",
   "metadata": {},
   "source": [
    "2.2. Train roBERTa Model\n",
    "\n",
    "We train with 5 Stratified KFolds (based on sentiment stratification). Each fold, the best model weights are saved and then reloaded before oof prediction and test prediction. Therefore you can run this code offline and upload your 5 fold models to a private Kaggle dataset. Then run this notebook and comment out the line model.fit(). Instead your notebook will load your model weights from offline training in the line model.load_weights(). Update this to have the correct path. Also make sure you change the KFold seed below to match your offline training. Then this notebook will proceed to use your offline models to predict oof and predict test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e046e-fcb7-4fa2-aee5-f9092d7b2cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file roBERTaFiles/input/config-roberta-base.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file roBERTaFiles/input/pretrained-roberta-base.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roBERTaFiles/input/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  7/687 [..............................] - ETA: 5:14:44 - loss: nan - activation_loss: nan - activation_1_loss: nan"
     ]
    }
   ],
   "source": [
    "jac = []\n",
    "VER='v0'\n",
    "DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be62865-581d-490c-af17-38065ae0c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5fbc9-11ed-45c1-87ff-f2fe4dbb3526",
   "metadata": {},
   "source": [
    "5. Use label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b8f71-257e-4423-9c35-4b531299e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ef0c47-85a4-46f6-9fba-da8272690cc8",
   "metadata": {},
   "source": [
    "6. Mask words\n",
    "\n",
    "Use data loader to randomly replace 5% of words with [mask] token 50264. Within your dataloader use the following code. We also maintain where the special tokens are so that they don't get replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f8ab3-47f3-4459-b879-6084379d7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.uniform(0,1,ids.shape)\n",
    "ids[r<0.05] = 50264 \n",
    "ids[tru] = self.ids[indexes][tru]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6440888-dbd7-4bb9-a562-53c0c93a8657",
   "metadata": {},
   "source": [
    "7. Decay learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e34ea-c7d1-40f5-93e9-19339334c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrfn(epoch):\n",
    "    dd = {0:4e-5,1:2e-5,2:1e-5,3:5e-6,4:2.5e-6}\n",
    "    return dd[epoch]\n",
    "lr = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b917f22e-9f20-4043-ba9e-84e856fea27a",
   "metadata": {},
   "source": [
    "8. Train each fold 100% data for submit\n",
    "\n",
    "After using normal 5 fold and early stopping, note how many epochs are optimal. Then for your LB submission, run your 5 folds with the fixed epoch number you found using 100% data each fold.\n",
    "\n",
    "9. Sample weight positive and negative\n",
    "\n",
    "In TensorFlow Keras it is easy to make certain training samples more important. The normal output from class DataGenerator(tf.keras.utils.Sequence) is (X,y). Instead output (X,y,w) where weight is the same shape as y. Then make w=2 for all the positive and negative targets and w=1 for all the neutral targets. Then train with the usual TensorFlow Keras calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b180e74-4d56-447a-8a1c-34e251481b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_gen = DataGenerator()\n",
    "model.fit(t_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad620f-242a-4e33-a878-1c627d9fdf09",
   "metadata": {},
   "source": [
    "10. Post process\n",
    "\n",
    "The above 9 changes already predict much of the noise. For example the above has no problem with the following 2 examples. Text is \" that's awesome!!!\" with selected text \"s awesome!\". And \" I'm thinking... wonderful.\" with selected text \". wonderful\". In each case, the model sees the leading double white space and extracts the single proceeding character.\n",
    "\n",
    "However the model cannot break a single letter off a word like text \"went fishing and loved it\" with selected text \"d loved\". This would require breaking a \"d\" off of the word \"and\". For these difficult cases, we use post process which increase CV 0.0025 and LB 0.0025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44e7d4-0ede-45ef-ac12-b6091ccc0b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT s=predicted, t=text, ex=sentiment\n",
    "# OUTPUT predicted with PP\n",
    "\n",
    "def applyPP(s,t,ex):\n",
    "\n",
    "    t1 = t.lower()\n",
    "    t2 = s.lower()\n",
    "\n",
    "    # CLEAN PREDICTED\n",
    "    b = 0\n",
    "    if len(t2)>=1:\n",
    "        if t2[0]==' ': \n",
    "            b = 1\n",
    "            t2 = t2[1:]\n",
    "    x = t1.find(t2)\n",
    "\n",
    "    # PREDICTED MUST BE SUBSET OF TEXT\n",
    "    if x==-1:\n",
    "        print('CANT FIND',k,x)\n",
    "        print(t1)\n",
    "        print(t2)\n",
    "        return s\n",
    "\n",
    "    # ADJUST FOR EXTRA WHITE SPACE\n",
    "    p = np.sum( np.array(t1[:x].split(' '))=='' )\n",
    "    if (p>2): \n",
    "        d = 0; f = 0\n",
    "        if p>3: \n",
    "            d=p-3\n",
    "        return t1[x-1-b-d:x+len(t2)]\n",
    "\n",
    "    # CLEAN BAD PREDICTIONS\n",
    "    if (len(t2)<=2)|(ex=='neutral'):\n",
    "        return t1\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba9d1b-da0b-4026-8f6a-b15b4847ae14",
   "metadata": {},
   "source": [
    "Other ideas\n",
    "\n",
    "Our team tried tons of more ideas which may have worked if we spent more time to refine them. Below are some interesting things we tried:\n",
    "\n",
    "replacing **** with the original curse word.\n",
    "using part of speech information as an additional feature\n",
    "using NER model predictions as additional features\n",
    "compare test text with train text using Jaccard and use train selected text when jac >= 0.85 and text length >= 4 . (This gained 0.001 on public LB but didn't change private LB).\n",
    "pretrain with Sentiment140 dataset as MLM (masked language model)\n",
    "pseudo label Sentiment140 dataset and pretrain as QA (question answer model)\n",
    "Train a BERT to choose the best prediction from multiple BERT predictions.\n",
    "Stack BERTs. Append output from one BERT to the QA training data of another BERT.\n",
    "Tons of ensembling ideas like Jaccard expectation, softmax manipulations, voting ensembles, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e741d84-3c95-421b-932f-3289bd2e128d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
