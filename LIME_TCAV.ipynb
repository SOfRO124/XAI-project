{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306c0ee9-6062-4d58-b028-536412acb5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.4 (v3.10.4:9d38120e33, Mar 23 2022, 17:29:05) [Clang 13.0.0 (clang-1300.0.29.30)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version) #should be <3.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f924f184-8e1e-436a-942a-1e883cf16a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 17:31:05.367199: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-30 17:31:20.579723: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4673197-989c-4ff3-8dc3-9d330cdf34d6",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7420b30-cfc5-412b-b3ac-316f5b81cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file roBERTaFiles/config-roberta-base.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file roBERTaFiles/pretrained-roberta-base.h5\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roBERTaFiles/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/content/drive/MyDrive/sentiment-extraction/pretrained-roberta-base.h5\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"begin_suppress_tokens\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"suppress_tokens\": null,\n",
      "  \"tf_legacy_loss\": false,\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 96)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 96)]         0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 96)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
      " odel)                          thPoolingAndCrossAt               'input_2[0][0]',                \n",
      "                                tentions(last_hidde               'input_3[0][0]']                \n",
      "                                n_state=(None, 96,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 96, 768)      0           ['tf_roberta_model_1[0][0]']     \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 96, 768)      0           ['tf_roberta_model_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 96, 1)        769         ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 96, 1)        769         ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 96, 1)        0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 96, 1)        0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 96)           0           ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 96)           0           ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 96)           0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 96)           0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124,647,170\n",
      "Trainable params: 124,647,170\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained('roBERTaFiles/config-roberta-base.json')\n",
    "bert_model = TFRobertaModel.from_pretrained('roBERTaFiles/pretrained-roberta-base.h5',config=config)\n",
    "\n",
    "new_model = tf.keras.models.load_model('v0-roberta-0l.h5',custom_objects={\"TFRobertaModel\": bert_model})\n",
    "\n",
    "# Show the model architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3802d3c-f129-4596-9432-76037daedd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'roBERTaFiles'\n",
    "DISPLAY=1\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "test = pd.read_csv('tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "# tokenizer\n",
    "#def tokenizer(sentiment_id=sentiment_id, test = test):\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab=PATH+'/vocab-roberta-base.json', \n",
    "    merges=PATH+'/merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "\n",
    "MAX_LEN = 96\n",
    "\n",
    "# test tweets\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]): \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1\n",
    "\n",
    "   # return [input_ids_t,attention_mask_t,token_type_ids_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdaded95-a946-43df-9745-e74a645e7b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(tokenizer()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1079e9f-f75d-49c9-98d4-0ec4383d5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model.predict([input_ids_t,attention_mask_t,token_type_ids_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2092d007-886b-4e6a-9bad-5fdfbc0febe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxV = 12\n",
    "# not working\n",
    "#oof_start[idxV,],oof_end[idxV,] = new_model.predict([input_ids_t[idxV,],attention_mask_t[idxV,],token_type_ids_t[idxV,]],verbose=DISPLAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adce6ed-0482-4c41-8aad-7cbe1dec8f0b",
   "metadata": {},
   "source": [
    "### LIME\n",
    "compare only with roberta sentiment part\n",
    "\n",
    "or with start/end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7408b617-4da6-4354-95c1-f07af39e3bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last session of the day  http://twitpic.com/67ezh\n"
     ]
    }
   ],
   "source": [
    "print(test['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c845270a-9dc4-4e75-a6c3-13d04524919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose layers to test with LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c1ff079-1dd5-4532-8e80-c7755008632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=sentiment_id.keys())\n",
    "\n",
    "#exp = explainer.explain_instance([input_ids_t[idxV,],attention_mask_t[idxV,],token_type_ids_t[idxV,]],new_model.predict, num_features=6)\n",
    "#exp.show_in_notebook(text=False)\n",
    "\n",
    "#exp1 = explainer.explain_instance(test.iloc[3], pipeline.predict_proba, num_features=6, top_labels=1)\n",
    "#exp1.show_in_notebook(text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8eee4-e59f-4826-8b34-9afd7f0215d9",
   "metadata": {},
   "source": [
    "### TCAV\n",
    "\n",
    "- first create lexicon for pos/neg/neutral sentiments\n",
    "- then use TCAV (captum) for projection on sentiments (concepts), probably after roBERTa layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3df6963-1c4b-454b-a93d-1279a5f80b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.datasets import LanguageModelingDataset\n",
    "\n",
    "from torchtext import vocab\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "#.... Captum imports..................\n",
    "from captum.concept import TCAV\n",
    "from captum.concept import Concept\n",
    "from captum.concept._utils.common import concepts_to_str\n",
    "from captum.concept._utils.data_iterator import dataset_to_dataloader, CustomIterableDataset\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ed8dcf0-2ac0-4d10-8972-363dc77a8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load VADER (https://github.com/cjhutto/vaderSentiment?tab=readme-ov-file#resources-and-dataset-descriptions) lexicon\n",
    "lexicon = pd.read_csv('tweet-sentiment-extraction/vader_lexicon.txt', sep = \"\\t\", header = None)\n",
    "\n",
    "# separate pos (>1), neg (<-1&<0), neu(>-1 & <1) based on polarity-scores (column 1)\n",
    "def convert_labels(elem):\n",
    "    if elem > 1.0:\n",
    "        return 'positive'\n",
    "    elif elem > -1.0 and elem < 1.0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "            \n",
    "\n",
    "lexicon[1] = lexicon[1].apply(lambda x: convert_labels(x))\n",
    "\n",
    "# create lexicon files for each sentiment\n",
    "lexicon = lexicon.drop(columns= [2,3])\n",
    "\n",
    "df_pos = lexicon[lexicon[1] == 'positive']\n",
    "df_neg = lexicon[lexicon[1] == 'negative']\n",
    "df_neu = lexicon[lexicon[1] == 'neutral']\n",
    "\n",
    "df_pos.to_csv('tweet-sentiment-extraction/positive.csv', index=False)\n",
    "df_neg.to_csv('tweet-sentiment-extraction/negative.csv', index=False)\n",
    "df_neu.to_csv('tweet-sentiment-extraction/neutral.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5a8d314-c100-4cf8-b341-f181ef062264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  7465\n"
     ]
    }
   ],
   "source": [
    "# make data field and load data\n",
    "def spacy_tok(x):\n",
    "    return [tok.text for tok in nlp.tokenizer(x)]\n",
    "\n",
    "TEXT = torchtext.data.Field(lower=True, tokenizer_language='en_core_web_sm', tokenize = spacy_tok)\n",
    "Label = torchtext.data.LabelField(dtype = torch.float)\n",
    "\n",
    "ds_pos = LanguageModelingDataset(\"tweet-sentiment-extraction/positive.csv\", TEXT)\n",
    "ds_neut = LanguageModelingDataset(\"tweet-sentiment-extraction/neutral.csv\", TEXT)\n",
    "ds_neg = LanguageModelingDataset(\"tweet-sentiment-extraction/negative.csv\", TEXT)\n",
    "\n",
    "TEXT.build_vocab(ds_pos,ds_neut,ds_neg)\n",
    "\n",
    "# load glove\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "loaded_vectors= torchtext.vocab.Vectors('tweet-sentiment-extraction/glove.6B.50d.txt')\n",
    "\n",
    "#TEXT.build_vocab(train, vectors=loaded_vectors, max_size=len(loaded_vectors.stoi))\n",
    "TEXT.vocab.set_vectors(stoi=loaded_vectors.stoi, vectors=loaded_vectors.vectors, dim=loaded_vectors.dim)\n",
    "print('Vocabulary Size: ', len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "168cf193-e58c-41ea-98e7-b27b03a63afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_from_filename(filename):\n",
    "    ds = torchtext.data.TabularDataset(path=filename,\n",
    "                                       fields=[('text', torchtext.data.Field()),\n",
    "                                               ('label', torchtext.data.Field())],\n",
    "                                       format='csv')\n",
    "    const_len = 7\n",
    "    for concept in ds:\n",
    "        concept.text = concept.text[:const_len]\n",
    "        concept.text += ['pad'] * max(0, const_len - len(concept.text))\n",
    "        text_indices = torch.tensor([TEXT.vocab.stoi[t] for t in concept.text], device=device)\n",
    "        yield text_indices\n",
    "        \n",
    "        \n",
    "def assemble_concept(name, id, concepts_path):\n",
    "    dataset = CustomIterableDataset(get_tensor_from_filename, concepts_path)\n",
    "    concept_iter = dataset_to_dataloader(dataset, batch_size=1)\n",
    "    return Concept(id=id, name=name, data_iter=concept_iter)\n",
    "\n",
    "\n",
    "def print_concept_sample(concept_iter):\n",
    "    cnt = 0\n",
    "    max_print = 10\n",
    "    item = next(concept_iter)\n",
    "    while cnt < max_print and item is not None:\n",
    "        print(' '.join([TEXT.vocab.itos[item_elem] for item_elem in item[0]]))\n",
    "        item = next(concept_iter)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b4e639c-9630-4ba1-90ee-5ba7cb6abeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create concepts: match glove embeddings to words in lexicon\n",
    "neut = assemble_concept('neutral', 0, concepts_path=\"tweet-sentiment-extraction/neutral.csv\")\n",
    "pos = assemble_concept('positive', 1, concepts_path=\"tweet-sentiment-extraction/positive.csv\")\n",
    "neg = assemble_concept('negative', 2, concepts_path=\"tweet-sentiment-extraction/negative.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16e80700-12dd-494c-acbd-734d85ff38fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "( <unk> ) <unk> <unk> <unk> <unk>\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "print_concept_sample(iter(pos.data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "956251ec-a873-426c-abbe-c3b31a987b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multiple neutral concepts to test \n",
    "# repeat with neg and pos\n",
    "neutral_concept = assemble_concept('neutral', 0, concepts_path=\"data/tcav/text-sensitivity/neutral.csv\")\n",
    "neutral_concept4 = assemble_concept('neutral4', 3, concepts_path=\"data/tcav/text-sensitivity/neutral4.csv\")\n",
    "neutral_concept5 = assemble_concept('neutral5', 4, concepts_path=\"data/tcav/text-sensitivity/neutral5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44a4585b-8eaa-4224-b1e8-32f294ac3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble sets to compare\n",
    "# repeat with diff layers\n",
    "# TCAV trains a model for each pair, and estimates tcav scores for each experimental set in given input layers.\n",
    "# score indicates the importance of a concept in a given layer (high = inportant)\n",
    "experimental_sets=[[pos, neutral_concept],\n",
    "                  [pos, neutral_concept2],\n",
    "                  [pos, neutral_concept3],\n",
    "                  [pos, neutral_concept4],\n",
    "                  [pos, neutral_concept5]]\n",
    "\n",
    "tcav = TCAV(new_model, layers=['tf_roberta_model_1', 'conv1d']) # change layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "048061a5-9e4e-4a67-b38c-22566168b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert concepts to tensors\n",
    "def covert_text_to_tensor(input_texts):\n",
    "    input_tensors = []\n",
    "    for input_text in input_texts:\n",
    "        input_tensor = torch.tensor([TEXT.vocab.stoi[tok.text] for \\\n",
    "                                     tok in nlp.tokenizer(input_text)], device=device).unsqueeze(0)\n",
    "        input_tensors.append(input_tensor)\n",
    "    return torch.cat(input_tensors)\n",
    "\n",
    "# show tcav scores \n",
    "def extract_scores(interpretations, layer_name, score_type, idx):\n",
    "    return [interpretations[key][layer_name][score_type][idx].item() for key in interpretations.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22e0d0b1-71cd-4b32-8dc9-b7ebfe2e53c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "There is more than one instance of a concept with id 1 defined in experimental sets. Please, make sure to reuse the same instance of concept",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m pos_input_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt was a fantastic play ! pad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA terrific film so far ! pad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe loved that show ! pad pad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m pos_input_text_indices \u001b[38;5;241m=\u001b[39m covert_text_to_tensor(pos_input_texts)\n\u001b[0;32m----> 5\u001b[0m positive_interpretations \u001b[38;5;241m=\u001b[39m \u001b[43mtcav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpret\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_input_text_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperimental_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperimental_sets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/captum/concept/_core/tcav.py:662\u001b[0m, in \u001b[0;36mTCAV.interpret\u001b[0;34m(self, inputs, experimental_sets, target, additional_forward_args, processes, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03mThis method computes magnitude and sign-based TCAV scores for each\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03mexperimental sets in `experimental_sets` list.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    655\u001b[0m \n\u001b[1;32m    656\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_to_layer_input\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs, (\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease, set `attribute_to_layer_input` flag as a constructor \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument to TCAV class. In that case it will be applied \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsistently to both layer activation and layer attribution methods.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    661\u001b[0m )\n\u001b[0;32m--> 662\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_cavs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperimental_sets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocesses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocesses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m scores: Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]]] \u001b[38;5;241m=\u001b[39m defaultdict(\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict()\n\u001b[1;32m    666\u001b[0m )\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# Retrieves the lengths of the experimental sets so that we can sort\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# them by the length and compute TCAV scores in batches.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/captum/concept/_core/tcav.py:495\u001b[0m, in \u001b[0;36mTCAV.compute_cavs\u001b[0;34m(self, experimental_sets, force_train, processes)\u001b[0m\n\u001b[1;32m    493\u001b[0m concept_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m concept \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcepts:\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m concept\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concept_ids, (\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is more than one instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof a concept with id \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m defined in experimental sets. Please, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure to reuse the same instance of concept\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    499\u001b[0m             \u001b[38;5;28mstr\u001b[39m(concept\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    502\u001b[0m     concept_ids\u001b[38;5;241m.\u001b[39mappend(concept\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_train:\n",
      "\u001b[0;31mAssertionError\u001b[0m: There is more than one instance of a concept with id 1 defined in experimental sets. Please, make sure to reuse the same instance of concept"
     ]
    }
   ],
   "source": [
    "# test tcav \n",
    "pos_input_texts = [\"It was a fantastic play ! pad\", \"A terrific film so far ! pad\", \"We loved that show ! pad pad\"]\n",
    "pos_input_text_indices = covert_text_to_tensor(pos_input_texts)\n",
    "\n",
    "positive_interpretations = tcav.interpret(pos_input_text_indices, experimental_sets=experimental_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002f362-20c4-450d-b66b-c9719e19ff32",
   "metadata": {},
   "source": [
    "#### visualize tcav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa41c038-70f5-43d4-9467-31832135722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_float(f):\n",
    "    return float('{:.3f}'.format(f) if abs(f) >= 0.0005 else '{:.3e}'.format(f))\n",
    "\n",
    "def plot_tcav_scores(experimental_sets, tcav_scores, layers = ['convs.2'], score_type='sign_count'):\n",
    "    fig, ax = plt.subplots(1, len(experimental_sets), figsize = (25, 7))\n",
    "\n",
    "    barWidth = 1 / (len(experimental_sets[0]) + 1)\n",
    "\n",
    "    for idx_es, concepts in enumerate(experimental_sets):\n",
    "        concepts = experimental_sets[idx_es]\n",
    "        concepts_key = concepts_to_str(concepts)\n",
    "        \n",
    "        layers = tcav_scores[concepts_key].keys()\n",
    "        pos = [np.arange(len(layers))]\n",
    "        for i in range(1, len(concepts)):\n",
    "            pos.append([(x + barWidth) for x in pos[i-1]])\n",
    "        _ax = (ax[idx_es] if len(experimental_sets) > 1 else ax)\n",
    "        for i in range(len(concepts)):\n",
    "            val = [format_float(scores[score_type][i]) for layer, scores in tcav_scores[concepts_key].items()]\n",
    "            _ax.bar(pos[i], val, width=barWidth, edgecolor='white', label=concepts[i].name)\n",
    "\n",
    "        # Add xticks on the middle of the group bars\n",
    "        _ax.set_xlabel('Set {}'.format(str(idx_es)), fontweight='bold', fontsize=16)\n",
    "        _ax.set_xticks([r + 0.3 * barWidth for r in range(len(layers))])\n",
    "        _ax.set_xticklabels(layers, fontsize=16)\n",
    "\n",
    "        # Create legend & Show graphic\n",
    "        _ax.legend(fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dd33da9-0c00-404c-8059-6664c8672f44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'positive_interpretations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# change inputs!!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plot_tcav_scores(experimental_sets, \u001b[43mpositive_interpretations\u001b[49m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_roberta_model_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv1d\u001b[39m\u001b[38;5;124m'\u001b[39m], score_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msign_count\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'positive_interpretations' is not defined"
     ]
    }
   ],
   "source": [
    "# change inputs!!\n",
    "plot_tcav_scores(experimental_sets, positive_interpretations, ['tf_roberta_model_1', 'conv1d'], score_type='sign_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4412f4-16e8-49e9-88db-efd31e112c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
