{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daoha\\Downloads\\explainableAI\\project\\tcav\n",
      "c:\\Users\\daoha\\Downloads\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.getcwd().split('/')[-1] != 'tcav':\n",
    "    print(os.getcwd())\n",
    "    os.chdir('../')\n",
    "    os.chdir('../')\n",
    "    os.chdir('../')    \n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcav.utils import create_session\n",
    "from tensorflow.io import gfile\n",
    "from tcav.model import ModelWrapper, KerasModelWrapper\n",
    "from tcav.tcav_examples.discrete.kdd99_activation_generator import KDD99DiscreteActivationGenerator\n",
    "from tcav.tcav_examples.discrete.kdd99_model_wrapper import KDD99KerasModelWrapper\n",
    "import tensorflow as tf\n",
    "from transformers import TFRobertaModel\n",
    "from tcav.activation_generator import ActivationGeneratorBase\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make source directory, \n",
    "source_dir = \"c://Users//daoha//Downloads//explainableAI//project//tcav//\"\n",
    "working_dir = source_dir\n",
    "acts_dir = os.path.join(working_dir, \"activations\")\n",
    "gfile.makedirs(acts_dir)\n",
    "cav_dir = os.path.join(working_dir, \"cav\")\n",
    "gfile.makedirs(cav_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 96\n",
    "PATH = 'c://Users//daoha//Downloads//explainableAI//project//tcav//roBERTaFiles//'\n",
    "\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab=PATH+'vocab-roberta-base.json',\n",
    "    merges=PATH+'merges-roberta-base.txt',\n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModelWrapper(ModelWrapper):\n",
    "    def __init__(self, sess, model_path, custom_objects=None):\n",
    "        self.sess = sess\n",
    "        super(SentimentModelWrapper, self).__init__()\n",
    "        self.import_keras_model(model_path, custom_objects=custom_objects)\n",
    "        self.optimiezer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        # Using SparseCategoricalCrossEntropy here\n",
    "        self.y_input = tf.compat.v1.placeholder(tf.int64, shape=[None])\n",
    "        self.y_true = tf.one_hot(self.y_input, depth=96)\n",
    "        self.loss = self.model.loss_functions[0](self.y_true, self.model.outputs[0])\n",
    "        self._make_gradient_tensors()\n",
    "\n",
    "    def import_keras_model(self, model_path, custom_objects=None):\n",
    "        \"Load the model and fetch input/output tensors.\"\n",
    "        self.ends = {}\n",
    "        self.model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "        self.get_bottleneck_tensors()\n",
    "        self.get_inputs_and_outputs_and_ends()\n",
    "\n",
    "    def get_bottleneck_tensors(self):\n",
    "        self.bottleneck_tensors = {}\n",
    "        bottleneck_layers = ['pooler_output', 'conv1d', 'conv1d_1']\n",
    "        for layer in bottleneck_layers:\n",
    "            if layer == 'pooler_output':\n",
    "                pooler_layer = self.model.get_layer('tf_roberta_model')\n",
    "                self.bottleneck_tensors[layer] = pooler_layer.output.pooler_output\n",
    "                print(type(self.bottleneck_tensors[layer]))\n",
    "            else:\n",
    "                self.bottleneck_tensors[layer] = self.model.get_layer(layer).output\n",
    "                print(type(self.bottleneck_tensors[layer]))\n",
    "\n",
    "    def get_inputs_and_outputs_and_ends(self):\n",
    "        \"Fetch input and output tensors.\"\n",
    "        self.ends['input'] = self.model.inputs[0]\n",
    "        self.ends['prediction'] = self.model.outputs[0]\n",
    "\n",
    "    def _make_gradient_tensors(self):\n",
    "        \"\"\"Makes gradient tensors for all bottleneck tensors.\"\"\"\n",
    "        print(self.bottleneck_tensors)\n",
    "        self.bottlenecks_gradients = {}\n",
    "        for bn_name, bn_tensor in self.bottleneck_tensors.items():\n",
    "            if bn_tensor is not None:\n",
    "                gradient_tensor = tf.gradients(\n",
    "                    ys=self.loss,\n",
    "                    xs=bn_tensor\n",
    "                )[0]\n",
    "                self.bottlenecks_gradients[bn_name] = gradient_tensor\n",
    "            else:\n",
    "                raise ValueError(f\"{bn_name} tensor is None. Check model architecture.\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcav.activation_generator import ActivationGeneratorBase\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class SentimentActivationGenerator(ActivationGeneratorBase):\n",
    "    def __init__(self, model, tokenizer, sentiment_id, source_dir, acts_dir, max_examples=500):\n",
    "        super(SentimentActivationGenerator, self).__init__(model, acts_dir, max_examples)\n",
    "        self.source_dir = source_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentiment_id = sentiment_id\n",
    "\n",
    "    def get_examples_for_concept(self, concept):\n",
    "        concept_folder = os.path.join(self.source_dir, concept)\n",
    "        concept_file = os.path.join(concept_folder, concept + '.csv')\n",
    "        df = pd.read_csv(concept_file)\n",
    "        examples = self.transform_data(df)\n",
    "        return examples\n",
    "\n",
    "    def transform_data(self, examples):\n",
    "        # Initialize the input arrays\n",
    "        input_ids = np.ones((len(examples), MAX_LEN), dtype='int32')\n",
    "        attention_mask = np.zeros((len(examples), MAX_LEN), dtype='int32')\n",
    "        token_type_ids_t = np.zeros((len(examples), MAX_LEN), dtype='int32') \n",
    "\n",
    "        # Tokenize the examples and set up the input arrays\n",
    "        for k in range(len(examples)):\n",
    "            text = \" \" + \" \".join(examples.loc[k,'text'].split())\n",
    "            enc = self.tokenizer.encode(text)\n",
    "            s_tok = self.sentiment_id[examples.loc[k,'sentiment']]\n",
    "            input_ids[k, :len(enc.ids)+5] = [0] + enc.ids + [2, 2] + [s_tok] + [2]\n",
    "            attention_mask[k, :len(enc.ids)+5] = 1\n",
    "\n",
    "        return [input_ids, attention_mask, token_type_ids_t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daoha\\miniconda3\\envs\\xai\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1793: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'tf_roberta_model/roberta/embeddings/assert_less/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"c:\\Users\\daoha\\miniconda3\\envs\\xai\\lib\\site-packages\\tensorflow\\python\\ops\\check_ops.py\", line 955, in assert_less\n",
      "    return _binary_assert('<', 'assert_less', math_ops.less, np.less, x, y, data,  File \"c:\\Users\\daoha\\miniconda3\\envs\\xai\\lib\\site-packages\\tensorflow\\python\\ops\\check_ops.py\", line 463, in _binary_assert\n",
      "    with ops.name_scope(name, opname, [x, y, data]):  File \"c:\\Users\\daoha\\miniconda3\\envs\\xai\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 155, in error_handler\n",
      "    del filtered_tb  File \"c:\\Users\\daoha\\miniconda3\\envs\\xai\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)  File \"c:\\Users\\daoha\\miniconda3\\envs\\xai\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(source_dir, \"v0-roberta-0.h5\")\n",
    "sess = create_session()\n",
    "custom_objects = {'TFRobertaModel': TFRobertaModel}\n",
    "\n",
    "mymodel = SentimentModelWrapper(sess, model_path, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create activation generator\n",
    "act_gen = SentimentActivationGenerator(model=mymodel, tokenizer=tokenizer, sentiment_id=sentiment_id, source_dir=source_dir, acts_dir=acts_dir, max_examples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:conv1d ['positive_words', 'random500_0'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['positive_words', 'random500_1'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['positive_words', 'random500_2'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['positive_words', 'random500_3'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['positive_words', 'random500_4'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['neutral_words', 'random500_0'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['neutral_words', 'random500_1'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['neutral_words', 'random500_2'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['neutral_words', 'random500_3'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['neutral_words', 'random500_4'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_0', 'random500_1'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_0', 'random500_2'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_0', 'random500_3'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_0', 'random500_4'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_1', 'random500_0'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_1', 'random500_2'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_1', 'random500_3'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_1', 'random500_4'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_2', 'random500_0'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_2', 'random500_1'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_2', 'random500_3'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_2', 'random500_4'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_3', 'random500_0'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_3', 'random500_1'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_3', 'random500_2'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_3', 'random500_4'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_4', 'random500_0'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_4', 'random500_1'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_4', 'random500_2'] positive 0.01\n",
      "INFO:tensorflow:conv1d ['random500_4', 'random500_3'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['positive_words', 'random500_0'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['positive_words', 'random500_1'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['positive_words', 'random500_2'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['positive_words', 'random500_3'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['positive_words', 'random500_4'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['neutral_words', 'random500_0'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['neutral_words', 'random500_1'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['neutral_words', 'random500_2'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['neutral_words', 'random500_3'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['neutral_words', 'random500_4'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_0', 'random500_1'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_0', 'random500_2'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_0', 'random500_3'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_0', 'random500_4'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_1', 'random500_0'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_1', 'random500_2'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_1', 'random500_3'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_1', 'random500_4'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_2', 'random500_0'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_2', 'random500_1'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_2', 'random500_3'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_2', 'random500_4'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_3', 'random500_0'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_3', 'random500_1'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_3', 'random500_2'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_3', 'random500_4'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_4', 'random500_0'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_4', 'random500_1'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_4', 'random500_2'] positive 0.01\n",
      "INFO:tensorflow:conv1d_1 ['random500_4', 'random500_3'] positive 0.01\n",
      "INFO:tensorflow:TCAV will 60 params\n",
      "INFO:tensorflow:running 60 params\n",
      "INFO:tensorflow:Running param 0 of 60\n",
      "INFO:tensorflow:running positive ['positive_words', 'random500_0']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 24\u001b[0m\n\u001b[0;32m     10\u001b[0m alphas \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.01\u001b[39m]\n\u001b[0;32m     14\u001b[0m my_tcav \u001b[38;5;241m=\u001b[39m TCAV(create_session,\n\u001b[0;32m     15\u001b[0m                    target,\n\u001b[0;32m     16\u001b[0m                    concepts,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m                    cav_dir\u001b[38;5;241m=\u001b[39mcav_dir,\n\u001b[0;32m     21\u001b[0m                    num_random_exp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmy_tcav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\daoha\\miniconda3\\envs\\xai\\lib\\site-packages\\tcav\\tcav.py:224\u001b[0m, in \u001b[0;36mTCAV.run\u001b[1;34m(self, num_workers, run_parallel, overwrite, return_proto)\u001b[0m\n\u001b[0;32m    222\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams):\n\u001b[0;32m    223\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning param \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (i, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)))\n\u001b[1;32m--> 224\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_single_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_parallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_parallel\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    225\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone running \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m params. Took \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams), time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m now))\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_proto:\n",
      "File \u001b[1;32mc:\\Users\\daoha\\miniconda3\\envs\\xai\\lib\\site-packages\\tcav\\tcav.py:255\u001b[0m, in \u001b[0;36mTCAV._run_single_set\u001b[1;34m(self, param, overwrite, run_parallel)\u001b[0m\n\u001b[0;32m    252\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (target_class, concepts))\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# Get acts\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m acts \u001b[38;5;241m=\u001b[39m \u001b[43mactivation_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_and_load_activations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mbottleneck\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcepts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# Get CAVs\u001b[39;00m\n\u001b[0;32m    258\u001b[0m cav_hparams \u001b[38;5;241m=\u001b[39m CAV\u001b[38;5;241m.\u001b[39mdefault_hparams()\n",
      "File \u001b[1;32mc:\\Users\\daoha\\miniconda3\\envs\\xai\\lib\\site-packages\\tcav\\activation_generator.py:82\u001b[0m, in \u001b[0;36mActivationGeneratorBase.process_and_load_activations\u001b[1;34m(self, bottleneck_names, concepts)\u001b[0m\n\u001b[0;32m     79\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     80\u001b[0m         acts_path, acts[concept][bottleneck_name]\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m   acts[concept][bottleneck_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_activations_for_concept\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconcept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottleneck_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m acts_path:\n\u001b[0;32m     85\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m does not exist, Making one...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(acts_path))\n",
      "File \u001b[1;32mc:\\Users\\daoha\\miniconda3\\envs\\xai\\lib\\site-packages\\tcav\\activation_generator.py:58\u001b[0m, in \u001b[0;36mActivationGeneratorBase.get_activations_for_concept\u001b[1;34m(self, concept, bottleneck)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_activations_for_concept\u001b[39m(\u001b[38;5;28mself\u001b[39m, concept, bottleneck):\n\u001b[0;32m     57\u001b[0m   examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_examples_for_concept(concept)\n\u001b[1;32m---> 58\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_activations_for_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottleneck\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\daoha\\miniconda3\\envs\\xai\\lib\\site-packages\\tcav\\activation_generator.py:61\u001b[0m, in \u001b[0;36mActivationGeneratorBase.get_activations_for_examples\u001b[1;34m(self, examples, bottleneck)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_activations_for_examples\u001b[39m(\u001b[38;5;28mself\u001b[39m, examples, bottleneck):\n\u001b[1;32m---> 61\u001b[0m   acts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottleneck\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mreshape_activations(acts)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32mc:\\Users\\daoha\\miniconda3\\envs\\xai\\lib\\site-packages\\tcav\\model.py:223\u001b[0m, in \u001b[0;36mModelWrapper.run_examples\u001b[1;34m(self, examples, bottleneck_name)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_examples\u001b[39m(\u001b[38;5;28mself\u001b[39m, examples, bottleneck_name):\n\u001b[0;32m    214\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Get activations at a bottleneck for provided examples.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    Activations in the given layer.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbottlenecks_tensors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbottleneck_name\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m    224\u001b[0m                        {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]: examples})\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from tcav.utils import create_session\n",
    "import tcav.utils_plot as utils_plot # utils_plot requires matplotlib\n",
    "from tcav.tcav import TCAV\n",
    "import absl\n",
    "absl.logging.set_verbosity(0)\n",
    "\n",
    "target = \"positive\"\n",
    "bottlenecks = [\"conv1d\", \"conv1d_1\"]\n",
    "concepts = [\"positive_words\", \"neutral_words\"]\n",
    "alphas = [0.01]\n",
    "\n",
    "\n",
    "\n",
    "my_tcav = TCAV(create_session,\n",
    "                   target,\n",
    "                   concepts,\n",
    "                   bottlenecks,\n",
    "                   act_gen,\n",
    "                   alphas,\n",
    "                   cav_dir=cav_dir,\n",
    "                   num_random_exp=5)\n",
    "\n",
    "\n",
    "results = my_tcav.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
